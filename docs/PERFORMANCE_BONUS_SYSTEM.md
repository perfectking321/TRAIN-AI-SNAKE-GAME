# ‚ö° Performance Bonus System

## üéØ Overview

While the AI Snake doesn't currently have explicit "performance bonus" rewards in the code, this document outlines **potential performance-based reward enhancements** you can add to encourage efficient gameplay and optimize learning speed.

Performance bonuses reward the AI for **how well** it plays, not just **what** it achieves.

---

## üí° Why Performance Bonuses?

### Current System Limitations
The existing reward system focuses on:
- ‚úÖ Eating food (+10)
- ‚úÖ Navigation efficiency (+1/-1)
- ‚úÖ Safety (tail avoidance)

**What's Missing?**
- Time efficiency (how fast food is collected)
- Move efficiency (shortest path to food)
- Consistency (maintaining high performance)
- Growth management (handling longer snakes better)

### Benefits of Performance Bonuses
1. **Faster learning**: Rewards optimal strategies more strongly
2. **Better gameplay**: Encourages human-like efficient movement
3. **Higher scores**: Optimizes for maximum food collection
4. **Smarter AI**: Learns to balance speed with safety

---

## üöÄ Proposed Performance Bonus Types

### 1. **Speed Bonus** ‚è±Ô∏è

Reward the AI for collecting food quickly.

#### Implementation
```python
# In game.py, add to __init__ or reset():
self.steps_since_food = 0

# In play_step(), before moving:
self.steps_since_food += 1

# When eating food:
if self.head == self.food:
    self.score += 1
    base_reward = 10
    
    # Speed bonus: faster collection = more reward
    if self.steps_since_food < 50:
        speed_bonus = (50 - self.steps_since_food) * 0.1
        reward = base_reward + speed_bonus
    else:
        reward = base_reward
    
    self.steps_since_food = 0  # Reset counter
```

#### Reward Formula
```
Speed Bonus = max(0, (50 - steps_taken) √ó 0.1)

Examples:
- 10 steps to food: +10 (food) + 4.0 (bonus) = 14.0
- 30 steps to food: +10 (food) + 2.0 (bonus) = 12.0
- 50+ steps to food: +10 (food) + 0.0 (bonus) = 10.0
```

---

### 2. **Efficiency Bonus** üìê

Reward the AI for taking near-optimal paths to food.

#### Implementation
```python
# In game.py, add helper method:
def _get_optimal_distance(self, start, end):
    """Manhattan distance (optimal path length)"""
    return abs(start.x - end.x) + abs(start.y - end.y)

# In play_step(), when eating food:
if self.head == self.food:
    optimal_distance = self._get_optimal_distance(
        self.prev_head_when_food_placed, 
        self.food
    )
    actual_distance = self.steps_since_food
    
    # Efficiency ratio
    efficiency = optimal_distance / max(actual_distance, 1)
    
    # Bonus for efficient pathing
    if efficiency > 0.8:  # Within 20% of optimal
        efficiency_bonus = efficiency * 2  # 0 to 2 points
        reward += efficiency_bonus
```

#### Reward Formula
```
Efficiency = optimal_path_length / actual_path_length
Efficiency Bonus = efficiency √ó 2 (if efficiency > 0.8)

Examples:
- Optimal path: 10 blocks, took 10 steps: efficiency = 1.0, bonus = +2.0
- Optimal path: 10 blocks, took 12 steps: efficiency = 0.83, bonus = +1.66
- Optimal path: 10 blocks, took 20 steps: efficiency = 0.5, bonus = +0.0 (too inefficient)
```

---

### 3. **Streak Bonus** üî•

Reward consecutive successful food collections to encourage consistency.

#### Implementation
```python
# In agent.py or game.py:
class SnakeGameAI:
    def __init__(self):
        # ... existing code ...
        self.food_streak = 0
        self.max_streak = 0

# In play_step(), when eating food:
if self.head == self.food:
    self.food_streak += 1
    self.max_streak = max(self.max_streak, self.food_streak)
    
    # Streak bonus (exponential)
    streak_bonus = min(self.food_streak * 0.5, 5)  # Cap at +5
    reward = 10 + streak_bonus

# On death:
if game_over:
    self.food_streak = 0
```

#### Reward Formula
```
Streak Bonus = min(consecutive_foods √ó 0.5, 5)

Examples:
- 1st food: +10 + 0.5 = 10.5
- 5th food: +10 + 2.5 = 12.5
- 10th food: +10 + 5.0 = 15.0 (capped)
- Death: streak resets to 0
```

---

### 4. **Length Bonus** üìè

Scale rewards based on snake length to reward managing longer snakes.

#### Implementation
```python
# In play_step(), when eating food:
if self.head == self.food:
    base_reward = 10
    
    # Length difficulty multiplier
    length_multiplier = 1.0 + (len(self.snake) / 100)
    reward = base_reward * length_multiplier
```

#### Reward Formula
```
Length Multiplier = 1.0 + (snake_length / 100)
Final Reward = base_reward √ó multiplier

Examples:
- Length 3: 10 √ó 1.03 = 10.3
- Length 10: 10 √ó 1.10 = 11.0
- Length 50: 10 √ó 1.50 = 15.0
```

---

### 5. **No-Danger Bonus** üõ°Ô∏è

Extra reward for collecting food without getting close to danger zones.

#### Implementation
```python
# Track if AI got close to tail during collection
class SnakeGameAI:
    def __init__(self):
        # ... existing code ...
        self.danger_encountered = False

# In play_step(), during movement:
if len(self.snake) > 3:
    min_tail_distance = self._get_min_distance_to_tail()
    if min_tail_distance <= 2 * BLOCK_SIZE:
        self.danger_encountered = True

# When eating food:
if self.head == self.food:
    reward = 10
    if not self.danger_encountered:
        reward += 3  # Clean collection bonus
    self.danger_encountered = False  # Reset for next food
```

---

## üìä Combined Example: Full Performance System

```python
def calculate_food_reward(self):
    """Calculate total reward for eating food with all bonuses"""
    base_reward = 10
    total_bonus = 0
    
    # 1. Speed bonus
    if self.steps_since_food < 50:
        speed_bonus = (50 - self.steps_since_food) * 0.1
        total_bonus += speed_bonus
    
    # 2. Efficiency bonus
    optimal_dist = self._get_optimal_distance(self.start_pos, self.food)
    efficiency = optimal_dist / max(self.steps_since_food, 1)
    if efficiency > 0.8:
        total_bonus += efficiency * 2
    
    # 3. Streak bonus
    streak_bonus = min(self.food_streak * 0.5, 5)
    total_bonus += streak_bonus
    
    # 4. Length multiplier
    length_multiplier = 1.0 + (len(self.snake) / 100)
    
    # 5. Clean collection bonus
    if not self.danger_encountered:
        total_bonus += 3
    
    final_reward = (base_reward + total_bonus) * length_multiplier
    return final_reward
```

### Example Calculations

**Scenario 1: Perfect Early Game**
- Food collected in 8 steps (optimal: 8)
- Length: 4
- No danger zones
- Streak: 3rd food

```
Base: 10
Speed bonus: (50-8) √ó 0.1 = 4.2
Efficiency: 8/8 = 1.0 ‚Üí 2.0
Streak: 3 √ó 0.5 = 1.5
Clean: +3.0
Subtotal: 10 + 4.2 + 2.0 + 1.5 + 3.0 = 20.7
Length multiplier: 1.04
Final: 20.7 √ó 1.04 = 21.5 points
```

**Scenario 2: Slow Late Game**
- Food collected in 60 steps (optimal: 20)
- Length: 40
- Danger zone entered
- Streak: 15th food

```
Base: 10
Speed bonus: 0 (too slow)
Efficiency: 20/60 = 0.33 ‚Üí 0 (below threshold)
Streak: 5.0 (capped)
Clean: 0 (entered danger)
Subtotal: 10 + 0 + 0 + 5.0 + 0 = 15.0
Length multiplier: 1.40
Final: 15.0 √ó 1.40 = 21.0 points
```

---

## ‚öôÔ∏è Implementation Guide

### Step 1: Add Tracking Variables
```python
class SnakeGameAI:
    def __init__(self):
        # ... existing code ...
        self.steps_since_food = 0
        self.food_streak = 0
        self.danger_encountered = False
        self.prev_head_when_food_placed = None
```

### Step 2: Update reset()
```python
def reset(self):
    # ... existing code ...
    self.steps_since_food = 0
    self.food_streak = 0
    self.danger_encountered = False
    self.prev_head_when_food_placed = self.head
```

### Step 3: Track During play_step()
```python
def play_step(self, action):
    self.steps_since_food += 1
    
    # ... movement code ...
    
    # Track danger zones
    if len(self.snake) > 3:
        if self._get_min_distance_to_tail() <= 2 * BLOCK_SIZE:
            self.danger_encountered = True
```

### Step 4: Calculate Rewards
```python
if self.head == self.food:
    reward = self.calculate_food_reward()  # Use bonus system
    self.steps_since_food = 0
    self.danger_encountered = False
    self.food_streak += 1
    self.prev_head_when_food_placed = self.head
```

---

## üîß Tuning Performance Bonuses

### If AI learns too slowly:
- **Increase speed bonus multiplier**: `0.1` ‚Üí `0.2`
- **Increase efficiency threshold**: `0.8` ‚Üí `0.6` (more lenient)
- **Increase streak bonus**: `0.5` ‚Üí `1.0` per food

### If AI is too aggressive:
- **Add danger penalty**: Reduce reward if danger encountered
- **Stricter efficiency**: `0.8` ‚Üí `0.9` threshold
- **Cap bonuses lower**: Max streak `5` ‚Üí `3`

### If rewards are too complex:
- **Start simple**: Implement only speed bonus
- **Add gradually**: Introduce one bonus at a time
- **Monitor training**: Check if complexity helps or hurts

---

## üìà Expected Impact

### Before Performance Bonuses:
- Food reward: Always +10
- AI treats all food equally
- No incentive for efficiency

### After Performance Bonuses:
- Food reward: +10 to +25 (depending on performance)
- AI learns optimal pathing faster
- Encourages consistent, safe play
- Higher scores in fewer training games

---

## ‚ö†Ô∏è Important Considerations

### 1. **Balance is Critical**
- Don't make bonuses too large (>50% of base reward)
- Ensure food is still rewarding even with poor bonuses
- Test extensively before committing

### 2. **Computational Cost**
- More tracking = more computation
- Keep calculations simple
- Avoid complex algorithms in tight loops

### 3. **Training Stability**
- Introduce bonuses gradually during training
- Monitor for unstable learning
- May need to adjust learning rate

### 4. **Interpretability**
- Complex reward functions are harder to debug
- Log bonus breakdowns for analysis
- Compare with baseline (no bonuses)

---

## üß™ Testing Performance Bonuses

### Baseline Test
```bash
# Train without bonuses
python train.py  # Note: mean score and convergence time
```

### Performance Bonus Test
```bash
# Train with bonuses implemented
python train.py  # Compare: mean score and convergence time
```

### Metrics to Track
- **Mean score by game 200**: Should be higher
- **First food at game**: Should be earlier
- **Score variance**: Should be lower (more consistent)
- **Training time**: May be longer but results better

---

## üí° Pro Tips

1. **Start with speed bonus only** - Simplest and most impactful
2. **Log bonus values** - Add to helper.py plotting
3. **Visualize bonuses** - Show breakdown in game UI
4. **A/B test** - Train two agents (with/without bonuses) and compare
5. **Adjust gradually** - Don't change multiple bonuses at once

---

## üî¨ Advanced: Adaptive Bonuses

Scale bonuses based on training progress:

```python
class Agent:
    def get_bonus_multiplier(self):
        """Reduce bonus importance as AI learns"""
        if self.n_games < 50:
            return 1.0  # Full bonuses early
        elif self.n_games < 150:
            return 0.5  # Reduce mid-training
        else:
            return 0.2  # Minimal bonuses late
```

This ensures:
- Early training: Strong guidance from bonuses
- Late training: Focus on fundamental rewards

---

## üìö Related Documentation

- [COMPLETE_REWARD_SYSTEM.md](./COMPLETE_REWARD_SYSTEM.md) - All base rewards
- [REWARD_SYSTEM_EXPLAINED.md](./REWARD_SYSTEM_EXPLAINED.md) - Learning mechanics
- [QUICK_TUNE_GUIDE.md](./QUICK_TUNE_GUIDE.md) - Practical tuning

---

## üöÄ Quick Start

To implement the simplest performance bonus (speed):

1. Add `self.steps_since_food = 0` to `__init__`
2. Add `self.steps_since_food += 1` to `play_step()`
3. Modify food reward:
```python
if self.head == self.food:
    speed_bonus = max(0, (50 - self.steps_since_food) * 0.1)
    reward = 10 + speed_bonus
    self.steps_since_food = 0
```

**That's it!** Train and observe if learning improves.

---

**Note**: This document describes a potential enhancement system, not current implemented features. Use as a guide for experimentation and optimization.
